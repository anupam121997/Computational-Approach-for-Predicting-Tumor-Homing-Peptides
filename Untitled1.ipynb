{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executed-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,load_model,save_model\n",
    "from keras.layers import Activation,Dropout,Flatten,Conv2D,MaxPooling2D,Dense,ActivityRegularization\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import os\n",
    "import keras\n",
    "from tensorflow import lite\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thousand-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dpc_train.pkl\", \"rb\") as fil:\n",
    "    train = pickle.load(fil)\n",
    "\n",
    "with open(\"dpc_test.pkl\", \"rb\") as fil:\n",
    "    test = pickle.load(fil)    \n",
    "fil.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "announced-distance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "analyzed-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranking-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [dat[:-1] for dat in train]\n",
    "y_train= [dat[-1:][0] for dat in train]\n",
    "\n",
    "x_test = [dat[:-1] for dat in test]\n",
    "y_test= [dat[-1:][0] for dat in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "arabic-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "x_test = np.asarray(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "rapid-southwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.30615577, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 , -0.43083952],\n",
       "       [ 6.73178608, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 , -0.43083952],\n",
       "       [ 0.82633813, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 , -0.43083952],\n",
       "       ...,\n",
       "       [-0.50238766, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 ,  1.61110468],\n",
       "       [-0.50238766, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 ,  2.56956829],\n",
       "       [-0.50238766, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 ,  0.59013258]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "wired-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "handmade-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73655865\n",
      "Iteration 2, loss = 0.55379691\n",
      "Iteration 3, loss = 0.45141110\n",
      "Iteration 4, loss = 0.37622437\n",
      "Iteration 5, loss = 0.31799624\n",
      "Iteration 6, loss = 0.27129418\n",
      "Iteration 7, loss = 0.23350888\n",
      "Iteration 8, loss = 0.20247048\n",
      "Iteration 9, loss = 0.17632204\n",
      "Iteration 10, loss = 0.15393042\n",
      "Iteration 11, loss = 0.13436677\n",
      "Iteration 12, loss = 0.11766295\n",
      "Iteration 13, loss = 0.10342622\n",
      "Iteration 14, loss = 0.09068101\n",
      "Iteration 15, loss = 0.07975326\n",
      "Iteration 16, loss = 0.07021453\n",
      "Iteration 17, loss = 0.06194670\n",
      "Iteration 18, loss = 0.05472300\n",
      "Iteration 19, loss = 0.04824290\n",
      "Iteration 20, loss = 0.04283823\n",
      "Iteration 21, loss = 0.03785961\n",
      "Iteration 22, loss = 0.03368934\n",
      "Iteration 23, loss = 0.03015877\n",
      "Iteration 24, loss = 0.02717504\n",
      "Iteration 25, loss = 0.02453156\n",
      "Iteration 26, loss = 0.02216527\n",
      "Iteration 27, loss = 0.02024537\n",
      "Iteration 28, loss = 0.01849433\n",
      "Iteration 29, loss = 0.01693723\n",
      "Iteration 30, loss = 0.01553266\n",
      "Iteration 31, loss = 0.01433337\n",
      "Iteration 32, loss = 0.01324349\n",
      "Iteration 33, loss = 0.01225522\n",
      "Iteration 34, loss = 0.01141051\n",
      "Iteration 35, loss = 0.01064359\n",
      "Iteration 36, loss = 0.00993227\n",
      "Iteration 37, loss = 0.00931504\n",
      "Iteration 38, loss = 0.00871263\n",
      "Iteration 39, loss = 0.00816397\n",
      "Iteration 40, loss = 0.00766139\n",
      "Iteration 41, loss = 0.00729409\n",
      "Iteration 42, loss = 0.00677231\n",
      "Iteration 43, loss = 0.00641097\n",
      "Iteration 44, loss = 0.00601014\n",
      "Iteration 45, loss = 0.00563995\n",
      "Iteration 46, loss = 0.00535740\n",
      "Iteration 47, loss = 0.00506706\n",
      "Iteration 48, loss = 0.00483438\n",
      "Iteration 49, loss = 0.00454376\n",
      "Iteration 50, loss = 0.00430205\n",
      "Iteration 51, loss = 0.00411076\n",
      "Iteration 52, loss = 0.00387992\n",
      "Iteration 53, loss = 0.00369524\n",
      "Iteration 54, loss = 0.00349830\n",
      "Iteration 55, loss = 0.00332342\n",
      "Iteration 56, loss = 0.00316250\n",
      "Iteration 57, loss = 0.00304645\n",
      "Iteration 58, loss = 0.00288218\n",
      "Iteration 59, loss = 0.00275688\n",
      "Iteration 60, loss = 0.00263425\n",
      "Iteration 61, loss = 0.00252788\n",
      "Iteration 62, loss = 0.00241733\n",
      "Iteration 63, loss = 0.00231388\n",
      "Iteration 64, loss = 0.00222690\n",
      "Iteration 65, loss = 0.00214285\n",
      "Iteration 66, loss = 0.00205825\n",
      "Iteration 67, loss = 0.00198559\n",
      "Iteration 68, loss = 0.00190599\n",
      "Iteration 69, loss = 0.00183825\n",
      "Iteration 70, loss = 0.00177308\n",
      "Iteration 71, loss = 0.00171338\n",
      "Iteration 72, loss = 0.00165370\n",
      "Iteration 73, loss = 0.00159780\n",
      "Iteration 74, loss = 0.00154454\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(64, 16), verbose=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MLPClassifier(activation='relu', hidden_layer_sizes=(64,16), verbose = 1)\n",
    "\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "referenced-rebel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy: 100.0\n",
      "Training Loss: 0.0015445404634364314\n",
      "Testing Accuracy: 84.86140724946695\n",
      "Testing Loss: 1.001234614618668\n"
     ]
    }
   ],
   "source": [
    "pred_train = classifier.predict(x_train)\n",
    "print(\"\\nTraining Accuracy:\",accuracy_score(pred_train, y_train)*100)\n",
    "print(\"Training Loss:\",classifier.loss_)\n",
    "\n",
    "pred_test = classifier.predict(x_test)\n",
    "prob = classifier.predict_proba(x_test)\n",
    "loss = log_loss(y_test,prob)\n",
    "print(\"Testing Accuracy:\",accuracy_score(pred_test, y_test)*100)\n",
    "print(\"Testing Loss:\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "premium-evolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.30615577, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 , -0.43083952],\n",
       "       [ 6.73178608, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 , -0.43083952],\n",
       "       [ 0.82633813, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 , -0.43083952],\n",
       "       ...,\n",
       "       [-0.50238766, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 ,  1.61110468],\n",
       "       [-0.50238766, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 ,  2.56956829],\n",
       "       [-0.50238766, -0.1888707 , -0.19518135, ..., -0.1392897 ,\n",
       "        -0.1055227 ,  0.59013258]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "failing-dover",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-a77be7f67521>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Training the ANN on the Training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1048\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1051\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    959\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[1;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m    962\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m         \"input: {}, {}\".format(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})"
     ]
    }
   ],
   "source": [
    "# Initializing the ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=40, activation='relu'))\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=15, activation='relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Part 3 - Training the ANN\n",
    "\n",
    "# Compiling the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Training the ANN on the Training set\n",
    "ann.fit(x_train, y_train, batch_size = 32, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-tyler",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-grass",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
